# Самостоятельные учебные работы по теме "Обработка естесвенного языка"

---
###  Задание: Классификация по тональности
В этом домашнем задании вам предстоит классифицировать по тональности отзывы на банки с сайта banki.ru.
Данные содержат непосредственно тексты отзывов, некоторую дополнительную информацию, а также оценку по шкале от 1 до 5.
Тексты хранятся в json-ах в массиве responses.
#### Часть 1. Анализ текстов
1. Посчитайте количество отзывов в разных городах и на разные банки
2. Постройте гистограмы длин слов в символах и в словах
3. Найдите 10 самых частых:
- слов
- слов без стоп-слов
- лемм
- существительных
4. Постройте кривые Ципфа и Хипса
5. Ответьте на следующие вопросы:
- какое слово встречается чаще, "сотрудник" или "клиент"?
- сколько раз встречается слова "мошенничество" и "доверие"?
#### Часть 2. Тематическое моделирование
1. Постройте несколько тематических моделей коллекции документов с разным числом тем. Приведите примеры понятных (интерпретируемых) тем.
2. Найдите темы, в которых упомянуты конкретные банки (Сбербанк, ВТБ, другой банк). Можете ли вы их прокомментировать / объяснить?
Эта часть задания может быть сделана с использованием gensim.
#### Часть 3. Классификация текстов
Сформулируем для простоты задачу бинарной классификации: будем классифицировать на два класса, то есть, различать резко отрицательные отзывы (с оценкой 1) и положительные отзывы (с оценкой 5).
1. Составьте обучающее и тестовое множество: выберите из всего набора данных N1 отзывов с оценкой 1 и N2 отзывов с оценкой 5 (значение N1 и N2 – на ваше усмотрение). Используйте sklearn.model_selection.train_test_split для разделения множества отобранных документов на обучающее и тестовое.
2. Используйте любой известный вам алгоритм классификации текстов для решения задачи и получите baseline. Сравните разные варианты векторизации текста: использование только униграм, пар или троек слов или с использованием символьных $n$-грам.
3. Сравните, как изменяется качество решения задачи при использовании скрытых тем в качестве признаков:
- 1-ый вариант: $tf-idf$ преобразование (sklearn.feature_extraction.text.TfidfTransformer) и сингулярное разложение (оно же – латентый семантический анализ) (sklearn.decomposition.TruncatedSVD),
- 2-ой вариант: тематические модели LDA (sklearn.decomposition.LatentDirichletAllocation).
Используйте accuracy и F-measure для оценки качества классификации.
Эта часть задания может быть сделана с использованием sklearn.
### [Ссылка на ноутбук в GoogleColab](https://colab.research.google.com/drive/1449J9cipl3xLNmBJIbBQ16hJahAtUEMR?usp=sharing)
---
###  Задание: Сделать классификацию данных fakenews
Используя данные fakenews, 3 раза разными способами получить на задаче классификации значение f1 выше 0.91 для методов на sklearn и выше 0.52 для методов на pytorch.
### [Ссылка на ноутбук в GoogleColab](https://colab.research.google.com/drive/1YCsVmd5CDaOvUN_H4v0uX4CCnWMwmOFa?usp=sharing)
---
###  Задание: Cоставить словари для классификации по тональности, получить навыки решения классических задач NLP
1. Cоставить словари для классификации по тональности. Данные для задания — отзывы на банки, собранные с нескольких сайтов рунета. Отзывы могут быть как положительными — оценка 5, так и отрицательными — оценка 1.
2. Разбейте всю коллекцию отзывов на предложения. Лемматизируйте все слова.
3. Обучите по коллекции предложений word2vec
4. Приведите несколько удачных и неудачных примеров решения стандартных текстов для word2vec:
- тест на определение ближайших слов
- тест на аналогии (мужчина — король : женщина — королева)
- тест на определение лишнего слова
5. Постройте несколько визуализаций:
- TSNE для топ-100 или топ-500 слов и найдите осмысленные кластеры слов
- задайте координаты для нового пространства следующим образом: одна ось описывает отношение «плохо — хорошо», вторая — «медленно — быстро», и найдите координаты названий банков в этих координатах
6. Распространить метку.
Определите 5–8 позитивных слов (например, быстрый, удобный) и 5–8 негативных слов (например, очередь, медленно). Эти слова будут основной будущего оценочного словаря
Пусть позитивному классу соответствует метка 1, негативному — -1
Пометьте выбранные слова в лексическом графе соответствующими метками
Запустите любой известный вам метод распространения метки (Label Propogation) в лексическом графе
На выходе метода распространения ошибки должны быть новые слова, помеченные метками 1 и -1 — это и есть искомые оценочные слова
 Результат:
Jupyter notebook с предобработанным текстом, аналитикой и выбранными топ-5–8 негативными и позитивными словами
Используя данные fakenews, 3 раза разными способами получить на задаче классификации значение f1 выше 0.91 для методов на sklearn и выше 0.52 для методов на pytorch.
### [Ссылка на ноутбук в GoogleColab](https://colab.research.google.com/drive/1k4Y7PbLkd2H383FFXTC_XDXwkVGzsQ0E?usp=sharing)
---
###  Задание: Обучите нейронную сеть решать шифр Цезаря
1. Написать алгоритм шифра Цезаря для генерации выборки (сдвиг на К каждой буквы. Например, при сдвиге на 2 буква “А” переходит в букву “В” и тп)
2. Сделать нейронную сеть
3. Обучить ее (вход - зашифрованная фраза, выход - дешифрованная фраза)
4. Проверить качество
5. Выполнить практическую работу из лекционного ноутбука.
  -Построить RNN-ячейку на основе полносвязных слоев
  -Применить построенную ячейку для генерации текста с выражениями героев сериала “Симпсоны”
### [Ссылка на ноутбук в GoogleColab](https://colab.research.google.com/drive/1spcBWGYyyLHMYQYTxdswd1K5uU8ORaMS?usp=sharing)
---
###  Задание: Решить задачу генерации элемента последовательности с помощью рекуррентных сетей
1. Сгенерировать последовательности, которые бы состояли из цифр (от 0 до 9) и задавались следующим образом:
   x - последовательность цифр 
   y1 = x1, y(i) = x(i) + x(1). Если y(i) >= 10, то y(i) = y(i) - 10

2. Научить модель предсказывать y(i) по x(i). Пробовать RNN, LSTM, GRU
3.* Применить LSTM для решения лекционного практического задания
### [Ссылка на ноутбук в GoogleColab](https://colab.research.google.com/drive/1j08xq9W1na4r5KueGa_FJcUucIF79eTM?usp=sharing)
---
###  Задание: Решить задачу перевода с помощью рекуррентных сетей
1. Возьмите англо-русскую пару фраз [www.manythings.org....org/anki/](https://www.manythings.org/anki/)
2. Обучите на них seq2seq по аналогии с занятием. Оцените полученное качество
3. Попробуйте добавить +1 рекуррентный слой в encoder и decoder
4. Попробуйте заменить GRU ячейки на lstm-ячейки
   Оцените качество во всех случаях
*5. Добавит в лекционный ноутбук, в функцию train, обучение батчами.
### [Ссылка на ноутбук в GoogleColab](https://colab.research.google.com/drive/1HUHtozC5CUjTLV--_lcXHEfYYp3Jn2xi?usp=sharing)
---
###  Задание: Решить задачу перевода с помощью механизма внимания
1. Возьмите англо-русскую пару фраз [www.manythings.org....org/anki/](https://www.manythings.org/anki/)
2. Обучите на них seq2seq with attention
a. На основе скалярного произведения
b. На основе MLP
3. Оцените качество
### [Ссылка на ноутбук в GoogleColab](https://colab.research.google.com/drive/1HUHtozC5CUjTLV--_lcXHEfYYp3Jn2xi?usp=sharing)
---
###  Задание: Работа с embeddings фраз
1. Cкачать датасет с фразами Симпсонов
2. Обучить word2vec на фразах персонажей
3. Визуализировать embeddings по самым частотным словам (top 1000)
4. Найти самые близкие слова для: homer - marge + bart bart - lisa + school marge - homer + home
5*. Попробовать построить классификатор bart/lisa с использованием этих эмбеддингов
### [Ссылка на ноутбук в GoogleColab](https://colab.research.google.com/drive/1L-Y-wiQ2yFJTMIEL8mPSOLF68_r4TRBW?usp=sharing)
---
### Задание: реализуйте задачу классификации на основе BERT-like модели и KNN на данных Russian Intents Dataset с Kaggle.
Цель: научиться создавать классификаторы текстов в условиях большого числа маленьких классов, состоящих из коротких текстов.
Результат: код для создания поискового векторного индекса + логика определения класса на основе близости к обучающим объектам (по ближайшему, по топ-N ближайших, и т. п.).
Инструменты: Python, приближённый KNN (nmslib/faiss/scann), модели из Hugging Face (Transformers).
### [Ссылка на ноутбук в GoogleColab](https://colab.research.google.com/drive/1WUuCWT7meLCxEh0-yM3TNDnd6D4w8w8g?usp=sharing)
---
